# Micro Stack Architecture
## Chapter 5: Building Infrastructure Stacks as Code

This directory contains **5 independent infrastructure stacks** implementing the **Micro Stack Pattern** from page 62 of "Infrastructure as Code" by Kief Morris.

---

## ğŸ¯ The Micro Stack Pattern

### What is it?

Breaking infrastructure into **small, independent components** that can be:
- Deployed separately
- Updated independently
- Tested in isolation
- Rolled back individually

### Why use it?

**Problem**: Monolithic stacks have a 100% blast radius. Change anything, risk everything.

**Solution**: Micro stacks reduce blast radius to 10-30% per change.

---

## ğŸ“Š Stack Overview

| Stack | File | Purpose | Blast Radius | Deploy Frequency |
|-------|------|---------|--------------|------------------|
| **1. Network** | `01-network-stack.yml` | VPC, Subnets, Security Groups | ~15% | Once (rarely) |
| **2. Database** | `02-database-stack.yml` | PostgreSQL, Storage | ~20% | Weekly |
| **3. Compute** | `03-compute-stack.yml` | EKS Cluster, Nodes | ~30% | Monthly |
| **4. Monitoring** | `04-monitoring-stack.yml` | Prometheus, Grafana | ~10% | Daily |
| **5. Application** | `05-application-stack.yml` | App, HPA, Services | ~15% | Hourly (CI/CD) |

---

## ğŸ—ï¸ Stack Dependencies

```
Network Stack (Foundation)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
Database Stack  Compute Stack  â”‚
    â”‚             â”‚             â”‚
    â”‚             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚             â”‚
    â”‚      Monitoring Stack
    â”‚             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  â”‚
          Application Stack
```

---

## ğŸš€ Usage

### Deploy All Stacks

```bash
cd /home/saul/Code/k8s-on-digital-ocean-main/ansible-aws
ansible-playbook deploy-all-stacks.yml
```

**Time**: 20-30 minutes (first deployment)

### Deploy Individual Stack

```bash
# Only Monitoring (lowest risk)
ansible-playbook deploy-all-stacks.yml --tags monitoring

# Only Application
ansible-playbook deploy-all-stacks.yml --tags application

# Database + Application
ansible-playbook deploy-all-stacks.yml --tags database,application
```

**Time**: 3-12 minutes per stack

### Skip Stacks

```bash
# Deploy all except network (already exists)
ansible-playbook deploy-all-stacks.yml --skip-tags network

# Deploy all except compute (cluster already created)
ansible-playbook deploy-all-stacks.yml --skip-tags compute
```

---

## ğŸ“‹ Stack Details

### Stack 1: Network Stack

**File**: `01-network-stack.yml`

**Components**:
- VPC (created by eksctl)
- Public/Private Subnets
- Internet Gateway
- NAT Gateways
- Route Tables
- Security Groups

**Note**: In our EKS setup, networking is managed by AWS. This stack **validates** rather than creates.

**Validation**:
```bash
aws eks describe-cluster --name k8s-autoscaling-cluster --region us-east-1
aws ec2 describe-vpcs --region us-east-1
```

---

### Stack 2: Database Stack

**File**: `02-database-stack.yml`

**Components**:
- PostgreSQL (Helm chart)
- StorageClass (EBS gp3)
- PersistentVolumes
- Database secrets
- Connection strings

**Deploy**:
```bash
ansible-playbook deploy-all-stacks.yml --tags database
```

**Verify**:
```bash
kubectl get pods -l app.kubernetes.io/name=postgresql
kubectl exec deployment/postgresdb-postgresql -- pg_isready
```

---

### Stack 3: Compute Stack

**File**: `03-compute-stack.yml`

**Components**:
- EKS Cluster (eksctl)
- Node Groups (2-5 t3.small)
- EBS CSI Driver
- Metrics Server
- Cluster Autoscaler
- Calico (Network Policies)

**Deploy**:
```bash
ansible-playbook deploy-all-stacks.yml --tags compute
```

**Verify**:
```bash
kubectl get nodes
kubectl top nodes
kubectl get pods -n kube-system
```

**â± Time**: 12-15 minutes (cluster creation)

---

### Stack 4: Monitoring Stack

**File**: `04-monitoring-stack.yml`

**Components**:
- Prometheus (kube-prometheus-stack)
- Grafana (with LoadBalancer)
- DORA Metrics Dashboard (Chapter 1)
- Drift Monitor CronJob (Chapter 2)
- ServiceMonitors

**Deploy**:
```bash
ansible-playbook deploy-all-stacks.yml --tags monitoring
```

**Access**:
```bash
# Get Grafana URL
kubectl get svc -n monitoring prometheus-grafana

# Get password
kubectl get secret -n monitoring prometheus-grafana \
  -o jsonpath="{.data.admin-password}" | base64 --decode
```

**â± Time**: 3-4 minutes

---

### Stack 5: Application Stack

**File**: `05-application-stack.yml`

**Components**:
- do-sample-app Deployment
- Service (LoadBalancer)
- HPA (1-10 pods, 50% CPU)
- Network Policies (Zero-Trust - Chapter 3)
- Locust load testing

**Deploy**:
```bash
ansible-playbook deploy-all-stacks.yml --tags application
```

**Verify**:
```bash
kubectl get pods -l app=do-sample-app
kubectl get hpa
curl http://$(kubectl get svc do-sample-app-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
```

**â± Time**: 2-3 minutes

---

## ğŸ§ª Testing Individual Stacks

### Test Monitoring Stack

```bash
# Deploy only monitoring
ansible-playbook deploy-all-stacks.yml --tags monitoring

# Verify Prometheus
kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus

# Verify Grafana
kubectl get svc -n monitoring prometheus-grafana

# Check DORA dashboard
kubectl get configmap dora-metrics-dashboard -n monitoring
```

### Test Application Stack

```bash
# Deploy only application
ansible-playbook deploy-all-stacks.yml --tags application

# Test application
APP_URL=$(kubectl get svc do-sample-app-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
curl http://$APP_URL

# Check HPA
kubectl get hpa do-sample-app-hpa

# Verify Network Policies
kubectl get networkpolicies
```

---

## ğŸ”„ Update Scenarios

### Scenario 1: Update Grafana Dashboard

**Before (Monolithic)**:
```bash
ansible-playbook deploy-eks-autoscaling.yml  # 15 min, 100% risk
```

**After (Micro Stack)**:
```bash
# Edit dashboard
vim ../manifests-aws/dora-metrics-dashboard.yaml

# Deploy only monitoring
ansible-playbook deploy-all-stacks.yml --tags monitoring  # 3 min, 10% risk
```

**Blast radius reduction**: 90%

---

### Scenario 2: Scale Application

**Before (Monolithic)**:
```bash
# Edit HPA
ansible-playbook deploy-eks-autoscaling.yml  # Re-checks everything
```

**After (Micro Stack)**:
```bash
# Edit HPA
vim ../manifests-aws/hpa.yaml

# Deploy only application
ansible-playbook deploy-all-stacks.yml --tags application  # 2 min
```

**Blast radius**: Only application affected

---

### Scenario 3: Upgrade Postgres

**Before (Monolithic)**:
```bash
ansible-playbook deploy-eks-autoscaling.yml  # Everything stops if DB fails
```

**After (Micro Stack)**:
```bash
# Update database
ansible-playbook deploy-all-stacks.yml --tags database  # Isolated

# If fails, only database affected
# Application keeps running on old DB
```

**Blast radius**: ~20% (database + dependent apps)

---

## ğŸ§¹ Cleanup

### Delete All Stacks

```bash
ansible-playbook cleanup-stacks.yml
```

âš ï¸ **WARNING**: This deletes ALL infrastructure!

**Deletion order** (reverse dependencies):
1. Application Stack
2. Monitoring Stack
3. Database Stack
4. Compute Stack (EKS cluster)
5. Network Stack (VPC)

**Time**: ~10-15 minutes

---

## ğŸ“Š Benefits Summary

| Metric | Monolithic | Micro Stacks | Improvement |
|--------|------------|--------------|-------------|
| **Blast Radius** | 100% | 10-30% | 70-90% reduction |
| **Deploy Time** | 15 min | 2-12 min | 20-85% faster |
| **Risk per Change** | High | Low | Significant |
| **Rollback Scope** | All | One stack | Isolated |
| **Team Ownership** | One team | Per stack | Scalable |
| **Testing** | Hard | Easy | Per stack |

---

## ğŸ“š Book References

### Direct Quotes

> **"Changing a large stack is riskier than changing a smaller stack. More things can go wrongâ€”it has a larger blast radius."**  
> â€” Page 62, Antipattern: Monolithic Stack

> **"A stack should be small enough that the team can understand it, and have confidence that they won't break things when they change it."**  
> â€” Page 63

> **"Breaking infrastructure into smaller pieces reduces the blast radius of each change."**  
> â€” Page 62

---

## ğŸ¯ Key Learnings

1. **Small stacks = Low risk**: Each stack affects only 10-30% of infrastructure
2. **Independent deployment**: Update one stack without touching others
3. **Faster iterations**: 2-3 min vs 15 min for typical changes
4. **Easier testing**: Test stacks in isolation
5. **Team scalability**: Different teams can own different stacks

---

## ğŸ”§ Troubleshooting

### Stack fails to deploy

```bash
# Check logs
ansible-playbook deploy-all-stacks.yml --tags monitoring -vv

# Verify dependencies
kubectl get nodes  # For monitoring/application
kubectl get pods -l app.kubernetes.io/name=postgresql  # For application
```

### Stack stuck in "Pending"

```bash
# Check events
kubectl get events --sort-by=.metadata.creationTimestamp

# Check resources
kubectl describe pod <pod-name>
```

### Want to rollback a stack

```bash
# Application stack
kubectl rollout undo deployment/do-sample-app

# Database stack (careful!)
helm rollback postgresdb

# Monitoring stack
helm rollback prometheus -n monitoring
```

---

## ğŸ“ Next Steps

1. **Practice**: Deploy individual stacks multiple times
2. **Experiment**: Change one stack, verify others unaffected
3. **Measure**: Compare deployment times vs monolithic
4. **Extend**: Add new stacks (e.g., Logging Stack, Backup Stack)
5. **Automate**: Add CI/CD for automated stack deployment

---

**Author**: Infrastructure Team  
**Date**: November 3, 2024  
**Book Reference**: Infrastructure as Code, Chapter 5 (Page 62)  
**Difficulty**: 50% (Architectural refactoring)
