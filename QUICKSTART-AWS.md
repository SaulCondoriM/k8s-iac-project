# üöÄ Gu√≠a R√°pida: AWS EKS Autoscaling

## Paso 1: Instalar Dependencias

```bash
# Instalar eksctl (necesita sudo)
./install-eksctl.sh

# Verificar otras dependencias
aws --version      # AWS CLI
kubectl version    # kubectl
helm version       # Helm
ansible --version  # Ansible
```

Si falta alguna, ver secci√≥n "Prerequisitos" en `README-AWS.md`

---

## Paso 2: Verificar Credenciales AWS

```bash
# Ver tu identidad AWS
aws sts get-caller-identity

# Verificar regi√≥n (debe ser us-east-1)
aws configure get region
```

---

## Paso 3: Desplegar Cluster EKS

### Opci√≥n A: Script Interactivo (Recomendado)

```bash
./aws-eks-manager.sh

# En el men√∫, selecciona:
# 1) Desplegar Cluster EKS Completo
```

### Opci√≥n B: Comandos Directos

```bash
./aws-eks-manager.sh deploy
```

**Tiempo**: 20-25 minutos

---

## Paso 4: Verificar Despliegue

```bash
# Ver estado completo
./aws-eks-manager.sh status

# O manualmente:
kubectl get nodes
kubectl get pods --all-namespaces
kubectl get hpa
```

---

## Paso 5: Ejecutar Prueba de Carga

```bash
# 1. Obtener URL de Locust
./aws-eks-manager.sh load-test

# 2. Abrir navegador en la URL mostrada
# 3. Configurar:
#    - Number of users: 100
#    - Spawn rate: 10
#    - Host: http://do-sample-app-service:8080

# 4. Click "Start swarming"

# 5. En otra terminal, monitorear:
./aws-eks-manager.sh monitor-hpa    # Ver escalado de pods
./aws-eks-manager.sh monitor-nodes  # Ver escalado de nodos
```

---

## Paso 6: Observar Autoscaling

### Escalado de Pods (HPA)

```bash
# Terminal 1: Ver HPA en tiempo real
kubectl get hpa -w

# Terminal 2: Ver pods
kubectl get pods -l app=do-sample-app -w

# Terminal 3: Ver m√©tricas
watch kubectl top pods -l app=do-sample-app
```

**Esperado**: 2 ‚Üí 4 ‚Üí 6 ‚Üí 8 ‚Üí 10 pods en 2-5 minutos

### Escalado de Nodos (Cluster Autoscaler)

```bash
# Terminal 1: Ver nodos
kubectl get nodes -w

# Terminal 2: Ver logs del autoscaler
kubectl logs -f deployment/cluster-autoscaler -n kube-system

# Terminal 3: Ver pods pending
watch 'kubectl get pods -l app=do-sample-app | grep Pending'
```

**Esperado**: 
- Pods en Pending cuando HPA escala m√°s all√° de la capacidad
- Nuevo nodo EC2 creado en 3-5 minutos
- Pods Pending se programan en el nuevo nodo

---

## Paso 7: Acceder a Grafana

```bash
# Obtener URL y contrase√±a
kubectl get svc prometheus-grafana -n monitoring

# Obtener contrase√±a
kubectl get secret prometheus-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d
echo

# Abrir navegador: http://<LOADBALANCER-URL>
# Usuario: admin
# Contrase√±a: <la del comando anterior>
```

**Dashboards recomendados**:
- Kubernetes / Compute Resources / Cluster
- Kubernetes / Compute Resources / Namespace (Pods)

---

## Paso 8: Limpiar Recursos

### Limpiar componentes pero mantener cluster:

```bash
./aws-eks-manager.sh cleanup
```

### Destruir completamente el cluster:

```bash
./aws-eks-manager.sh destroy
```

‚ö†Ô∏è **Esto eliminar√° TODO y es irreversible**

---

## üìä Resultados Esperados

### Escalado Exitoso de Pods

```
Tiempo    | Carga    | Pods | CPU   | Estado
----------|----------|------|-------|------------------
T+0       | Ninguna  | 2    | ~5%   | Inicial
T+1min    | Media    | 4    | ~60%  | Escalando
T+2min    | Alta     | 6    | ~55%  | Escalando
T+3min    | Alta     | 8    | ~50%  | Cerca del target
T+5min    | Alta     | 10   | ~50%  | Estable (max)
```

### Escalado Exitoso de Nodos

```
Tiempo    | Nodos | Estado de Pods      | Acci√≥n
----------|-------|---------------------|----------------------
T+0       | 2     | 2 running           | Inicial
T+2min    | 2     | 6 running           | HPA escal√≥, caben
T+3min    | 2     | 8 running, 2 pending| Sin recursos
T+4min    | 2     | Logs: "scale-up"    | CA detecta necesidad
T+7min    | 3     | 10 running          | Nuevo nodo creado ‚úÖ
```

---

## üêõ Problemas Comunes

### Error: "cluster not found"

```bash
# Verificar que el cluster existe
eksctl get cluster --region us-east-1

# Si no existe, cr√©alo primero
cd ansible-aws
eksctl create cluster -f cluster-config.yaml
```

### Error: "unauthorized" al acceder al cluster

```bash
# Reconfigurar kubectl
aws eks update-kubeconfig --name k8s-autoscaling-cluster --region us-east-1
```

### Pods en "Pending" mucho tiempo

```bash
# Ver por qu√© est√° pending
kubectl describe pod <pod-name>

# Verificar logs del Cluster Autoscaler
kubectl logs -f deployment/cluster-autoscaler -n kube-system
```

### LoadBalancer en "Pending"

```bash
# Ver eventos del servicio
kubectl describe svc <service-name>

# Esperar 2-3 minutos (es normal que tarde)
```

---

## üí∞ Estimaci√≥n de Costos

### Durante las Pruebas (1-2 horas)

- **Con Free Tier**: $0-2 USD
- **Sin Free Tier**: $0.50-1 USD

### Si lo dejas corriendo 24h

- **Con Free Tier**: ~$3-5 USD/d√≠a
- **Sin Free Tier**: ~$5-8 USD/d√≠a

‚ö†Ô∏è **IMPORTANTE**: Destruye el cluster cuando no lo uses para evitar cargos.

```bash
./aws-eks-manager.sh destroy
```

---

## üìö Archivos Importantes

| Archivo | Descripci√≥n |
|---------|-------------|
| `aws-eks-manager.sh` | Script principal de gesti√≥n |
| `README-AWS.md` | Documentaci√≥n completa |
| `ansible-aws/cluster-config.yaml` | Configuraci√≥n del cluster |
| `ansible-aws/deploy-eks-autoscaling.yml` | Playbook de despliegue |
| `manifests-aws/cluster-autoscaler.yaml` | Pol√≠ticas de autoscaling de nodos |
| `manifests-aws/hpa.yaml` | Pol√≠ticas de autoscaling de pods |

---

## ‚úÖ Checklist de Verificaci√≥n

Antes de empezar:
- [ ] AWS CLI instalado y configurado
- [ ] eksctl instalado
- [ ] kubectl instalado
- [ ] Helm instalado
- [ ] Ansible instalado
- [ ] Credenciales AWS verificadas (`aws sts get-caller-identity`)
- [ ] Regi√≥n configurada en us-east-1

Despu√©s del despliegue:
- [ ] Cluster EKS creado (`kubectl get nodes`)
- [ ] Pods corriendo (`kubectl get pods -A`)
- [ ] HPA configurado (`kubectl get hpa`)
- [ ] Cluster Autoscaler corriendo (`kubectl get deployment -n kube-system cluster-autoscaler`)
- [ ] LoadBalancers asignados (`kubectl get svc -A | grep LoadBalancer`)

Durante las pruebas:
- [ ] HPA escala pods correctamente
- [ ] Cluster Autoscaler crea nuevos nodos cuando es necesario
- [ ] Scale down funciona despu√©s de quitar carga
- [ ] Grafana muestra m√©tricas correctamente

---

## üéØ Pr√≥ximos Pasos

1. **Explorar Grafana**: Ver dashboards de Kubernetes
2. **Experimentar con carga**: Probar diferentes configuraciones en Locust
3. **Modificar pol√≠ticas**: Cambiar thresholds en HPA y Cluster Autoscaler
4. **Ver costos**: Revisar AWS Cost Explorer
5. **Limpiar**: Destruir recursos cuando termines

---

**¬øProblemas?** Revisa `README-AWS.md` secci√≥n "Troubleshooting"

**¬øDudas sobre costos?** Revisa `README-AWS.md` secci√≥n "Costos Estimados"
