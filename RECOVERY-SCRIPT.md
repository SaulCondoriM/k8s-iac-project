# üîÑ Script de Recuperaci√≥n Autom√°tica

## Descripci√≥n
Script para recuperar autom√°ticamente la aplicaci√≥n cuando se cae debido a problemas de conexi√≥n con PostgreSQL durante pruebas de carga.

## Uso

### Opci√≥n 1: Ejecuci√≥n Simple
```bash
./recover-app.sh
```

### Opci√≥n 2: Con logs detallados
```bash
./recover-app.sh 2>&1 | tee recovery-$(date +%Y%m%d-%H%M%S).log
```

## ¬øQu√© hace el script?

1. ‚úÖ **Verifica** conectividad con el cluster de Kubernetes
2. üîÑ **Reinicia** PostgreSQL para limpiar conexiones corruptas
3. ‚è≥ **Espera** a que PostgreSQL est√© completamente operativo
4. üóëÔ∏è **Elimina** todos los pods de la aplicaci√≥n con conexiones problem√°ticas
5. üöÄ **Recrea** pods nuevos con conexiones frescas
6. üîç **Verifica** que la aplicaci√≥n responda correctamente
7. üìä **Muestra** el estado final de todos los componentes

## Tiempo estimado
- **Total**: ~1-2 minutos
- PostgreSQL reinicio: 30-40 segundos
- Pods de aplicaci√≥n: 20-30 segundos
- Verificaciones: 15-20 segundos

## Cu√°ndo usarlo

### ‚úÖ Usar cuando:
- La aplicaci√≥n retorna 502 Bad Gateway
- Despu√©s de pruebas de carga intensas con Locust
- Ves errores "conn busy" en los logs
- Los pods est√°n en CrashLoopBackOff

### ‚ùå No usar cuando:
- El cluster de Kubernetes no est√° accesible
- Hay problemas de red con DigitalOcean
- Los nodos del cluster est√°n ca√≠dos

## Verificaci√≥n Manual

Despu√©s de ejecutar el script, puedes verificar manualmente:

```bash
# Ver estado de los pods
kubectl get pods

# Ver logs de la aplicaci√≥n
kubectl logs -l app=do-sample-app --tail=20

# Probar la aplicaci√≥n
curl http://45.55.116.144/

# Ver m√©tricas del HPA
kubectl get hpa do-sample-app-hpa
```

## Monitoreo en Tiempo Real

```bash
# Ver pods en tiempo real
kubectl get pods -l app=do-sample-app -w

# Ver HPA en tiempo real
kubectl get hpa -w

# Ver logs en streaming
kubectl logs -f -l app=do-sample-app
```

## Troubleshooting

### El script falla en PostgreSQL
```bash
# Verificar estado de PostgreSQL
kubectl get pods -l app.kubernetes.io/name=postgresql
kubectl logs postgresdb-postgresql-0 --tail=50

# Reinicio manual
kubectl delete pod postgresdb-postgresql-0
```

### Los pods no se levantan
```bash
# Ver qu√© est√° pasando
kubectl describe pods -l app=do-sample-app

# Ver eventos del cluster
kubectl get events --sort-by='.lastTimestamp' | tail -20
```

### La aplicaci√≥n a√∫n retorna 502
```bash
# Esperar m√°s tiempo (a veces necesita 30-60 segundos)
sleep 30
curl http://45.55.116.144/

# O ejecutar el script de nuevo
./recover-app.sh
```

## Automatizaci√≥n con Cron

Para ejecutar autom√°ticamente cada X minutos (no recomendado en producci√≥n):

```bash
# Editar crontab
crontab -e

# Agregar (ejecutar cada 10 minutos si falla)
*/10 * * * * /ruta/a/recover-app.sh >> /var/log/k8s-recovery.log 2>&1
```

## Notas Importantes

‚ö†Ô∏è **Este script es un workaround temporal**. El problema real es un bug en el c√≥digo de la aplicaci√≥n que no maneja correctamente el pool de conexiones de PostgreSQL.

### Soluci√≥n Permanente Recomendada:
1. Modificar el c√≥digo Go para usar un pool de conexiones (`pgxpool`)
2. Implementar retry logic para reconexiones
3. Agregar circuit breakers
4. Implementar health checks apropiados

## URLs de Acceso

- **Aplicaci√≥n**: http://45.55.116.144/
- **Locust UI**: http://138.197.240.205:8089
- **Grafana**: `kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80`

## Soporte

Si el script no funciona despu√©s de 2-3 intentos:
1. Verificar logs del cluster
2. Revisar cuotas de recursos en DigitalOcean
3. Considerar escalar los nodos del cluster
4. Verificar que no haya problemas de red

---

**√öltima actualizaci√≥n**: 25 de octubre de 2025
